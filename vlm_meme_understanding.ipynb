{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Memes are, arguably, the best proving ground for Vision Language Models (VLMs) because they combine multiple challenging aspects of visual and linguistic understanding:\n",
        "\n",
        "1. Memes require understanding both visual elements and text, and crucially, how they interact\n",
        "\n",
        "2. They often rely on shared cultural knowledge or references\n",
        "\n",
        "3. The humor often emerges from subtle interactions between the image and text\n",
        "\n",
        "4. They appear in different formats, templates, and styles\n",
        "\n",
        "5. Text can appear in various fonts, sizes, and positions\n",
        "\n",
        "In this highly rigorous, academic, journal quality blog post, I'll put two VLMs (Janus-Pro and Moondream2) through their paces on three distinct tasks:\n",
        "\n",
        "\n",
        "1. OCR: Can they accurately extract text from memes?\n",
        "\n",
        "2. Meme Understanding: Can they explain what makes a meme funny and relevant?\n",
        "\n",
        "3. Caption Generation: Can they generate contextually appropriate, humorous captions?\n",
        "\n",
        "I'll also test their attention to detail by seeing if they can spot subtle watermarks, giving us insight into their visual processing capabilities.\n",
        "\n",
        "Start by setting up our environment and downloading the necessary plugins:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9CLCxs4MP24"
      },
      "outputs": [],
      "source": [
        "!pip install fiftyone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I've created plugins which allow you to easily use [üåîMoondream2](https://github.com/harpreetsahota204/moondream2-plugin) and [üêãJanus-Pro](https://github.com/harpreetsahota204/janus-vqa-fiftyone) with your FiftyOne dataset.\n",
        "\n",
        "Let's start by downloading the plugins and installing their dependencies.\n",
        "\n",
        "> The plugin framework lets you extend and customize the functionality of FiftyOne to suit your needs.  If you‚Äôre interested in learning more about plugins, you might be interested in attending one of our monthly workshops. You can [see the full schedule here](https://voxel51.com/computer-vision-events/) and look for the *Advanced Computer Vision Data Curation and Model Evaluation workshop*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd74M4nZMUWX"
      },
      "outputs": [],
      "source": [
        "!fiftyone plugins download https://github.com/harpreetsahota204/janus-vqa-fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWgglxjzLZJA"
      },
      "outputs": [],
      "source": [
        "!fiftyone plugins requirements @harpreetsahota/janus_vqa --install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCKj4BhFMUSp"
      },
      "outputs": [],
      "source": [
        "!fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFjRW8OoLbz2"
      },
      "outputs": [],
      "source": [
        "!fiftyone plugins requirements @harpreetsahota/moondream2 --install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to set an enviornment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oX1jZcOfn1qC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I found [this webiste - scott.ai from Scott Penberthy](https://scott.ai/2019-08-06-memeified-ng) which had some awesome machine learning memes on it. I parsed these meme's into a FiftyOne dataset. You can download the dataset [from Hugging Face](https://huggingface.co/datasets/harpreetsahota/memes-dataset) as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avIS3aghMUZ5"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "from fiftyone.utils import huggingface as fouh\n",
        "\n",
        "ml_memes_dataset = fouh.load_from_hub(\n",
        "    \"harpreetsahota/ml-memes\",\n",
        "    name=\"ml-memes\",\n",
        "    overwrite=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's quickly explore the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x18zFcFQK2C"
      },
      "outputs": [],
      "source": [
        "fo.launch_app(ml_memes_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dAVhHbNPKJu"
      },
      "source": [
        "Now, let's instantiate our plugions as operators via the FiftyOne SDK.\n",
        "\n",
        "Alternatively, you can use the app and fill out the operator form. Just hit the backtick button (`) to open the operator menu. Type in ‚ÄúMoondream‚Äù or \"Janus\" and click on it. You'll be presented with a form to fill out, which takes the same information as what we will pass in via the SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvEU33CzO5Qt"
      },
      "outputs": [],
      "source": [
        "import fiftyone.operators as foo\n",
        "\n",
        "janus_vqa = foo.get_operator(\"@harpreetsahota/janus_vqa/janus_vqa\")\n",
        "\n",
        "moondream = foo.get_operator(\"@harpreetsahota/moondream2/moondream\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's kick off a delegated service by opening the terminal and running `fiftyone delegated launch`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPrq4sKRMUdB"
      },
      "source": [
        "# OCR\n",
        "\n",
        "Optical Character Recognition (OCR) is a fundamental task in Computer Vision.\n",
        "\n",
        "And, I think, using it to parse test from memes is a good use case! Memes typically combine both visual elements and text. While traditional OCR systems are specifically trained for text extraction, it's interesting to test how well general-purpose Vision Language Models (VLMs) can perform this task.\n",
        "\n",
        "Testing VLMs on OCR helps us understand:\n",
        "\n",
        "1. Their ability to perceive and accurately read text in various fonts, orientations, and styles common in memes\n",
        "\n",
        "2. How well they can distinguish between text and visual elements\n",
        "\n",
        "3. Their robustness in handling text that's integrated into images rather than presented as plain text\n",
        "\n",
        "Let's test both Janus-Pro and Moondream2 on this task using the plugins we downloaded earlier.\n",
        "\n",
        "First, let's run Janus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tTJDzXcOOC2"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"What does the text on this image say? Respond only with the text on the image and nothing else.\"\n",
        "\n",
        "await janus_vqa(\n",
        "    ml_memes_dataset,\n",
        "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
        "    question=QUESTION,\n",
        "    question_field=\"ocr_questions\",\n",
        "    answer_field=\"janus_ocr\",\n",
        "    delegate=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgqDSoCNT0-0"
      },
      "source": [
        "And now, Moondream2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0kwC9zUOOH9"
      },
      "outputs": [],
      "source": [
        "await moondream(\n",
        "    ml_memes_dataset,\n",
        "    revision=\"2025-01-09\",\n",
        "    operation=\"query\",\n",
        "    output_field=\"moondream_ocr\",\n",
        "    query_text=QUESTION,\n",
        "    delegate=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we don't have ground truth annotations for the text in these memes, we'll do a qualitative evaluation - a \"vibe check\" - of how well each model performs. \n",
        "\n",
        "We can visually inspect the results in the FiftyOne App by comparing the model outputs against the actual meme images to assess accuracy and completeness of text extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGNgKrUo2lDG"
      },
      "outputs": [],
      "source": [
        "fo.launch_app(ml_memes_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN2m_1i2EqAb"
      },
      "source": [
        "# Meme understanding\n",
        "\n",
        "Understanding memes is more complex than OCR, because there are multiple levels of comprehension:\n",
        "\n",
        "1. Recognizing the scene, characters, and their expressions\n",
        "\n",
        "2. Understanding the reference or template being used\n",
        "\n",
        "3. Connecting how the text relates to the visual elements\n",
        "\n",
        "4. Grasping why the combination is meant to be humorous\n",
        "\n",
        "This means we can test a VLMs ability to:\n",
        "\n",
        "- Integrate multimodal information (text and visuals)\n",
        "\n",
        "- Understand cultural references and context\n",
        "\n",
        "- Grasp abstract concepts and humor\n",
        "\n",
        "- Explain complex social/cultural phenomena in natural language\n",
        "\n",
        "Let's see how our models handle this deeper level of understanding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMoAYW1HEuWV"
      },
      "outputs": [],
      "source": [
        "MEME_UNDERSTANDING_QUESTION = \"\"\"This image is a meme. Describe the scene of the meme,\n",
        "its characters, what they are saying, and what the\n",
        "target audience of this meme might find funny about it.\n",
        "\"\"\"\n",
        "\n",
        "await janus_vqa(\n",
        "    ml_memes_dataset,\n",
        "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
        "    question=MEME_UNDERSTANDING_QUESTION,\n",
        "    question_field=\"meme_understanding_question\",\n",
        "    answer_field=\"janus_meme_understanding\",\n",
        "    delegate=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2bx39Z0Euaw"
      },
      "outputs": [],
      "source": [
        "await moondream(\n",
        "    ml_memes_dataset,\n",
        "    revision=\"2025-01-09\",\n",
        "    operation=\"query\",\n",
        "    output_field=\"moondream_meme_understanding\",\n",
        "    query_text=MEME_UNDERSTANDING_QUESTION,\n",
        "    delegate=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PafBHPYFklOB"
      },
      "outputs": [],
      "source": [
        "fo.launch_app(ml_memes_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Can the VLMs find the attribution tag?\n",
        "\n",
        "Each meme has a small attribution in the left corner, which reads `@scott.ai`. This presents an interesting test case for VLMs' visual capabilities because:\n",
        "\n",
        "1. The attribution is intentionally subtle - a small watermark that could be easily missed even by human viewers\n",
        "\n",
        "2. It tests the model's ability to detect and read fine details in images\n",
        "\n",
        "3. It evaluates whether VLMs can distinguish between the main meme content and metadata like attributions\n",
        "\n",
        "4. It helps us understand if VLMs can maintain attention to small details while processing the broader image\n",
        "\n",
        "This kind of test is particularly relevant for real-world applications where models might need to:\n",
        "- Detect watermarks or copyright information\n",
        "- Read small print or disclaimers\n",
        "- Identify subtle branding elements\n",
        "\n",
        "Let's see if the VLMs can pick up on this subtle detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ATTR_QUESTION = \"\"\"The creator of this meme has tagged themselves for self-attribution. \n",
        "Who can we attribute as the creator of this meme? Respond with just the authors name\"\"\"\n",
        "\n",
        "await janus_vqa(\n",
        "    ml_memes_dataset,\n",
        "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
        "    question=ATTR_QUESTION,\n",
        "    question_field=\"attr_question\",\n",
        "    answer_field=\"janus_attr\",\n",
        "    delegate=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await moondream(\n",
        "    ml_memes_dataset,\n",
        "    revision=\"2025-01-09\",\n",
        "    operation=\"query\",\n",
        "    output_field=\"moondream_attr\",\n",
        "    query_text=ATTR_QUESTION,\n",
        "    delegate=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fo.launch_app(ml_memes_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANl5h0TSVxqg"
      },
      "source": [
        "## Now let's test these models on captioning\n",
        "\n",
        "Meme captioning is a generative task that's distinct from our previous experiments:\n",
        "\n",
        "- Unlike OCR, which extracts existing text\n",
        "\n",
        "- Unlike meme understanding, which interprets the combined meaning\n",
        "\n",
        "- Captioning requires the model to create novel, contextually appropriate text\n",
        "\n",
        "This is challenging because good meme captions:\n",
        "\n",
        "1. Match the visual template's intended use\n",
        "\n",
        "2. Are culturally relevant to the target audience (in our case, the ML/AI community)\n",
        "\n",
        "3. Strike a right balance of humor and relatability\n",
        "\n",
        "4. Follow an established format of the meme template\n",
        "\n",
        "You could use metrics like BLEU or ROUGE to evaluate captions against references, but they often miss the aspects of humor and cultural relevance. Like the previous tasks, a qualitative \"vibe check\" is probably the most reliable way to assess the quality of the captions.\n",
        "\n",
        "Let's download another dataset (which is a grouped dataset, with a captioned and uncaptioned meme...but we will work with only uncaptioned) and then see what our models generate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HhJu369avhj"
      },
      "outputs": [],
      "source": [
        "memes_dataset = fouh.load_from_hub(\n",
        "    \"harpreetsahota/memes-dataset\",\n",
        "    name=\"meme-captioning\",\n",
        "    overwrite=True\n",
        "    )\n",
        "\n",
        "uncaptioned_memes = memes_dataset.select_group_slices(\"template\")\n",
        " \n",
        "uncaptioned_memes = uncaptioned_memes.clone(name=\"vlm-captioned-memes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plgjeTOXzzR0"
      },
      "outputs": [],
      "source": [
        "fo.launch_app(uncaptioned_memes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o-_FtvO4avlt"
      },
      "outputs": [],
      "source": [
        "MEME_GENERATE = \"\"\"This image is a meme. Write a caption for this meme that is\n",
        "realted to deep learning and artificial intelligence.\n",
        "Respond only with the caption and nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe7qe9GgeH0I"
      },
      "outputs": [],
      "source": [
        "await janus_vqa(\n",
        "    uncaptioned_memes,\n",
        "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
        "    question=MEME_GENERATE,\n",
        "    question_field=\"caption_prompt\",\n",
        "    answer_field=\"janus_caption\",\n",
        "    delegate=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv8TUSeTavps"
      },
      "outputs": [],
      "source": [
        "await moondream(\n",
        "    uncaptioned_memes,\n",
        "    revision=\"2025-01-09\",\n",
        "    operation=\"query\",\n",
        "    query_text=MEME_GENERATE,\n",
        "    output_field=\"moondream_caption\",\n",
        "    delegate=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOWsqdzxTpA9"
      },
      "outputs": [],
      "source": [
        "fo.launch_app(uncaptioned_memes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fiftyone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
